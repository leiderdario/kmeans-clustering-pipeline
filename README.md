# ğŸ¯ K-means Clustering Pipeline - UI/UX Mejorada

[![Python](https://img.shields.io/badge/Python-3.7%2B-blue)](https://www.python.org/)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-1.0%2B-orange)](https://scikit-learn.org/)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)
[![GitHub](https://img.shields.io/github/stars/leiderdario/kmeans-clustering-pipeline?style=social)](https://github.com/leiderdario/kmeans-clustering-pipeline)

## ğŸ“‹ DescripciÃ³n

Pipeline completo de **anÃ¡lisis de clustering con K-means** con interfaz de usuario mejorada. Implementa todas las etapas del aprendizaje automÃ¡tico con visualizaciones profesionales y reportes HTML interactivos.

### âœ¨ CaracterÃ­sticas Principales

1. âœ… **SelecciÃ³n y Carga de Datos** - CSV o DataFrame
2. ğŸ”§ **PreparaciÃ³n AutomÃ¡tica** - ImputaciÃ³n, codificaciÃ³n, normalizaciÃ³n
3. ğŸ“Š **AnÃ¡lisis Exploratorio (EDA)** - EstadÃ­sticas, correlaciones, distribuciones
4. ğŸ¯ **ReducciÃ³n Dimensional (PCA)** - SimplificaciÃ³n inteligente
5. ğŸ¤– **K-means Clustering** - BÃºsqueda automÃ¡tica de K Ã³ptimo
6. ğŸ¨ **Visualizaciones** - GrÃ¡ficos 2D/3D de alta calidad
7. ğŸ“„ **Reporte HTML** - Dashboard interactivo con resultados

### ğŸš€ **Nuevo: UI/UX Mejorada**

- âœ… Sin ventanas emergentes que bloquean la ejecuciÃ³n
- âœ… Mensajes con colores y emojis contextuales
- âœ… OrganizaciÃ³n automÃ¡tica de resultados
- âœ… Reporte HTML profesional
- âœ… Encoding UTF-8 para Windows

---

## ğŸš€ InstalaciÃ³n

### Requisitos previos
- Python 3.7 o superior
- pip (gestor de paquetes de Python)

### InstalaciÃ³n de dependencias

```bash
pip install -r requirements.txt
```

O instalar manualmente:
```bash
pip install numpy pandas matplotlib seaborn scikit-learn scipy
```

---

## ğŸ’» Uso del Script

### OpciÃ³n 1: Ejecutar con datos de ejemplo

El script incluye un generador de datos sintÃ©ticos para demostraciÃ³n:

```bash
python kmeans_clustering.py
```

### OpciÃ³n 2: Usar tus propios datos CSV

```python
from kmeans_clustering import run_kmeans_pipeline

# Pipeline automÃ¡tico completo
analyzer = run_kmeans_pipeline(filepath='tus_datos.csv')
```

### OpciÃ³n 3: Control paso a paso (avanzado)

```python
from kmeans_clustering import KMeansAnalyzer

# Crear analizador
analyzer = KMeansAnalyzer(random_state=42)

# 1. Cargar datos
analyzer.load_data(filepath='tus_datos.csv')

# 2. Preparar datos (eliminar columnas innecesarias)
analyzer.prepare_data(drop_columns=['ID', 'Timestamp'])

# 3. AnÃ¡lisis exploratorio
analyzer.exploratory_analysis()

# 4. ReducciÃ³n de dimensionalidad (opcional)
analyzer.reduce_dimensions(variance_threshold=0.95)

# 5. Encontrar K Ã³ptimo
analyzer.find_optimal_k(k_range=range(2, 10))

# 6. Entrenar modelo
analyzer.fit_kmeans(n_clusters=4, use_pca=True)

# 7. Visualizar clusters
analyzer.visualize_clusters(use_pca=True)

# 8. Analizar caracterÃ­sticas
cluster_stats = analyzer.analyze_clusters()

# 9. Guardar resultados
analyzer.save_results('datos_clusterizados.csv')
```

---

## ğŸ“ Estructura del Proyecto

```
K-means/database1/
â”‚
â”œâ”€â”€ kmeans_clustering.py      # Script principal
â”œâ”€â”€ requirements.txt           # Dependencias
â”œâ”€â”€ ejemplo_datos.csv          # Dataset de ejemplo
â”œâ”€â”€ README.md                  # Este archivo
â”‚
â””â”€â”€ Salidas generadas:
    â”œâ”€â”€ correlation_matrix.png       # Matriz de correlaciÃ³n
    â”œâ”€â”€ distributions.png            # Distribuciones de variables
    â”œâ”€â”€ outliers_boxplot.png         # DetecciÃ³n de outliers
    â”œâ”€â”€ pca_variance.png             # AnÃ¡lisis de varianza PCA
    â”œâ”€â”€ optimal_k_analysis.png       # BÃºsqueda de K Ã³ptimo
    â”œâ”€â”€ clusters_2d.png              # VisualizaciÃ³n 2D
    â”œâ”€â”€ clusters_3d.png              # VisualizaciÃ³n 3D
    â”œâ”€â”€ cluster_features.png         # CaracterÃ­sticas por cluster
    â”œâ”€â”€ cluster_analysis.csv         # EstadÃ­sticas de clusters
    â””â”€â”€ clustered_data.csv           # Datos con etiquetas
```

---

## ğŸ” CaracterÃ­sticas Principales

### 1. SelecciÃ³n y Carga de Datos
- âœ… Carga desde CSV o DataFrame
- âœ… ValidaciÃ³n automÃ¡tica de estructura
- âœ… InformaciÃ³n detallada del dataset

### 2. PreparaciÃ³n de Datos
- ğŸ”§ Manejo de valores faltantes (imputaciÃ³n automÃ¡tica)
- ğŸ”§ CodificaciÃ³n de variables categÃ³ricas (Label Encoding)
- ğŸ”§ NormalizaciÃ³n (StandardScaler: Î¼=0, Ïƒ=1)
- ğŸ”§ IdentificaciÃ³n automÃ¡tica de tipos de variables

### 3. AnÃ¡lisis Exploratorio (EDA)
- ğŸ“Š EstadÃ­sticas descriptivas completas
- ğŸ“Š Matriz de correlaciÃ³n con heatmap
- ğŸ“Š Distribuciones de todas las variables
- ğŸ“Š DetecciÃ³n de outliers con boxplots
- ğŸ“Š Visualizaciones de alta calidad

### 4. SimplificaciÃ³n de Datos (PCA)
- ğŸ¯ ReducciÃ³n de dimensionalidad automÃ¡tica
- ğŸ¯ SelecciÃ³n de componentes por varianza explicada
- ğŸ¯ VisualizaciÃ³n de varianza acumulada
- ğŸ¯ TransformaciÃ³n reversible

### 5. Clustering K-means
- ğŸ¤– BÃºsqueda automÃ¡tica de K Ã³ptimo
- ğŸ¤– MÃ©todo del codo automatizado
- ğŸ¤– MÃºltiples mÃ©tricas de evaluaciÃ³n:
  - Coeficiente de Silueta
  - Davies-Bouldin Index
  - Calinski-Harabasz Index
  - Inercia
- ğŸ¤– Entrenamiento robusto con mÃºltiples inicializaciones

### 6. VisualizaciÃ³n de Resultados
- ğŸ¨ GrÃ¡ficos 2D y 3D de clusters
- ğŸ¨ VisualizaciÃ³n de centroides
- ğŸ¨ AnÃ¡lisis de caracterÃ­sticas por cluster
- ğŸ¨ Todas las imÃ¡genes en alta resoluciÃ³n (300 DPI)

---

## ğŸ“Š MÃ©tricas de EvaluaciÃ³n

### Coeficiente de Silueta
- **Rango:** -1 a 1
- **Mejor:** Valores cercanos a 1
- **InterpretaciÃ³n:** Mide quÃ© tan similar es un objeto a su propio cluster comparado con otros clusters

### Davies-Bouldin Index
- **Rango:** 0 a âˆ
- **Mejor:** Valores mÃ¡s bajos
- **InterpretaciÃ³n:** Mide la separaciÃ³n entre clusters (valores bajos = mejor separaciÃ³n)

### Calinski-Harabasz Index
- **Rango:** 0 a âˆ
- **Mejor:** Valores mÃ¡s altos
- **InterpretaciÃ³n:** Ratio de dispersiÃ³n entre clusters vs dentro de clusters

### Inercia (Within-Cluster Sum of Squares)
- **Rango:** 0 a âˆ
- **Mejor:** Valores mÃ¡s bajos
- **InterpretaciÃ³n:** Suma de distancias cuadradas de muestras a su centroide mÃ¡s cercano

---

## ğŸ“ Conceptos Clave

### Â¿QuÃ© es K-means?
K-means es un algoritmo de **aprendizaje no supervisado** que agrupa datos similares en K clusters. Cada observaciÃ³n pertenece al cluster con el centroide mÃ¡s cercano.

### Pasos del Algoritmo:
1. Inicializar K centroides aleatoriamente
2. Asignar cada punto al centroide mÃ¡s cercano
3. Recalcular centroides como el promedio de puntos asignados
4. Repetir pasos 2-3 hasta convergencia

### Â¿CuÃ¡ndo usar K-means?
- âœ… SegmentaciÃ³n de clientes
- âœ… CompresiÃ³n de imÃ¡genes
- âœ… DetecciÃ³n de anomalÃ­as
- âœ… AnÃ¡lisis de patrones
- âœ… AgrupaciÃ³n de documentos

---

## ğŸ“ Ejemplo PrÃ¡ctico

### Dataset de ejemplo: `ejemplo_datos.csv`

Contiene datos de clientes con las siguientes variables:
- **Age:** Edad del cliente
- **Income:** Ingreso anual
- **SpendingScore:** PuntuaciÃ³n de gasto (0-100)
- **MembershipYears:** AÃ±os de membresÃ­a
- **VisitsPerMonth:** Visitas mensuales
- **Category:** CategorÃ­a de cliente (Premium/Standard)
- **Region:** RegiÃ³n geogrÃ¡fica

### Resultado Esperado

El script automÃ¡ticamente:
1. Identifica 4 clusters Ã³ptimos
2. Segmenta clientes por comportamiento
3. Genera visualizaciones explicativas
4. Proporciona estadÃ­sticas por cluster
5. Guarda resultados en archivos CSV y PNG

---

## âš™ï¸ ParÃ¡metros Personalizables

### KMeansAnalyzer()
```python
KMeansAnalyzer(random_state=42)  # Semilla para reproducibilidad
```

### prepare_data()
```python
prepare_data(
    target_column='etiqueta',      # Columna objetivo a excluir
    drop_columns=['ID', 'Fecha']   # Columnas a eliminar
)
```

### reduce_dimensions()
```python
reduce_dimensions(
    n_components=3,           # NÃºmero de componentes (None = automÃ¡tico)
    variance_threshold=0.95   # Varianza mÃ­nima a retener (95%)
)
```

### find_optimal_k()
```python
find_optimal_k(
    k_range=range(2, 11),    # Rango de K a evaluar
    use_pca=True             # Usar datos reducidos por PCA
)
```

### fit_kmeans()
```python
fit_kmeans(
    n_clusters=4,            # NÃºmero de clusters (None = automÃ¡tico)
    use_pca=True             # Usar datos reducidos
)
```

---

## ğŸ› ï¸ SoluciÃ³n de Problemas

### Error: "No module named 'sklearn'"
```bash
pip install scikit-learn
```

### Error: "No se pueden crear grÃ¡ficos"
- Verifica instalaciÃ³n de matplotlib
- En entornos sin GUI, usa: `matplotlib.use('Agg')`

### Advertencia: "Convergence warning"
- Normal en datasets complejos
- Aumenta `max_iter` en KMeans si persiste

### Dataset muy grande
- Activa `use_pca=True` para reducir dimensionalidad
- Considera muestreo aleatorio antes del anÃ¡lisis

---

## ğŸ“š Referencias y Recursos

### DocumentaciÃ³n Oficial
- [Scikit-learn K-means](https://scikit-learn.org/stable/modules/clustering.html#k-means)
- [PCA en Scikit-learn](https://scikit-learn.org/stable/modules/decomposition.html#pca)
- [Pandas](https://pandas.pydata.org/docs/)
- [Matplotlib](https://matplotlib.org/stable/contents.html)

### ArtÃ­culos AcadÃ©micos
- MacQueen, J. (1967). "Some methods for classification and analysis of multivariate observations"
- Arthur, D., & Vassilvitskii, S. (2007). "k-means++: The advantages of careful seeding"

---

## ğŸ‘¨â€ğŸ’» Autor

**Sistema de AnÃ¡lisis ML**  
Desarrollado para el SENA - Programa de Aprendizaje AutomÃ¡tico  
Fecha: Octubre 2025

---

## ğŸ“„ Licencia

Este proyecto es de cÃ³digo abierto y estÃ¡ disponible para fines educativos.

---

## ğŸ¤ Contribuciones

Â¿Encontraste un bug o tienes una sugerencia?
- Crea un issue con descripciÃ³n detallada
- Fork el proyecto y envÃ­a pull requests
- Comparte tus casos de uso

---

## â­ CaracterÃ­sticas Adicionales Futuras

- [ ] Soporte para clustering jerÃ¡rquico
- [ ] IntegraciÃ³n con DBSCAN y HDBSCAN
- [ ] ExportaciÃ³n a formatos interactivos (Plotly)
- [ ] Dashboard web con Streamlit
- [ ] API REST para predicciones
- [ ] OptimizaciÃ³n con GPU (RAPIDS)

---

**Â¡Disfruta del anÃ¡lisis de clustering! ğŸš€ğŸ“Š**
